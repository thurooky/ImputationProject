{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\r\n",
    "\r\n",
    "import time \r\n",
    "import numpy\r\n",
    "import scipy\r\n",
    "from scipy import sparse\r\n",
    "import torch\r\n",
    "import pandas\r\n",
    "import pyBigWig\r\n",
    "\r\n",
    "from tqdm import tqdm\r\n",
    "from torchsummary import summary\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "torch.backends.cudnn.benchmark = True\r\n",
    "\r\n",
    "#@torch.jit.script\r\n",
    "def MLLLoss(logps, true_counts):\r\n",
    "\t\"\"\" Adapted from Alex.\r\n",
    "\t\"\"\"\r\n",
    "\t# Multinomial probability = n! / (x1!...xk!) * p1^x1 * ... pk^xk\r\n",
    "\t# Log prob = log(n!) - (log(x1!) ... + log(xk!)) + x1log(p1) ... + xklog(pk)\r\n",
    "\r\n",
    "\tlog_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\r\n",
    "\tlog_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\r\n",
    "\tlog_prod_exp = torch.sum(true_counts * logps, dim=-1) \r\n",
    "\treturn -torch.mean(log_fact_sum - log_prod_fact + log_prod_exp)\r\n",
    "\r\n",
    "def pearson_corr(arr1, arr2):\r\n",
    "\t\"\"\"The Pearson correlation between two draws of samples.\r\n",
    "\r\n",
    "\tThis function is more efficient than the built-in `corrcoef` function\r\n",
    "\tbecause it only calculates the pairwise correlations between elements\r\n",
    "\tin arr1 and arr2 rather than the correlation between all elements in arr1\r\n",
    "\tand arr2, and does so in a vectorized manner.\r\n",
    "\r\n",
    "\tComputes the Pearson correlation in the last dimension of `arr1` and `arr2`.\r\n",
    "\t`arr1` and `arr2` must be the same shape. For example, if they are both\r\n",
    "\tA x B x L arrays, then the correlation of corresponding L-arrays will be\r\n",
    "\tcomputed and returned in an A x B array.\r\n",
    "\r\n",
    "\tParameters\r\n",
    "\t----------\r\n",
    "\tarr1: numpy.ndarray, shape=(A, ..., L)\r\n",
    "\t\tAn array of any dimensionality > 1 as long as the last dimension\r\n",
    "\t\tcorresponds to a sample from the same distribution.\r\n",
    "\r\n",
    "\tarr2: numpy.ndarray, shape=(A, ..., L)\r\n",
    "\t\tAn array of any dimensionality > 1 as long as the last dimension\r\n",
    "\t\tcorresponds to a sample from the same distribution.\t\r\n",
    "\r\n",
    "\tReturns\r\n",
    "\t-------\r\n",
    "\tcorr : numpy.array, shape=(A, ...)\r\n",
    "\t\tThe Pearson correlation along the last dimension.\r\n",
    "\t\"\"\"\r\n",
    "\r\n",
    "\tmean1 = numpy.mean(arr1, axis=-1, keepdims=True)\r\n",
    "\tmean2 = numpy.mean(arr2, axis=-1, keepdims=True)\r\n",
    "\tdev1, dev2 = arr1 - mean1, arr2 - mean2\r\n",
    "\tsqdev1, sqdev2 = numpy.square(dev1), numpy.square(dev2)\r\n",
    "\tnumer = numpy.sum(dev1 * dev2, axis=-1)  # Covariance\r\n",
    "\tvar1, var2 = numpy.sum(sqdev1, axis=-1), numpy.sum(sqdev2, axis=-1)  # Variances\r\n",
    "\tdenom = numpy.sqrt(var1 * var2)\r\n",
    "   \r\n",
    "\t# Divide numerator by denominator, but use NaN where the denominator is 0\r\n",
    "\treturn numpy.divide(\r\n",
    "\t\tnumer, denom, out=numpy.full_like(numer, numpy.nan), where=(denom != 0)\r\n",
    "\t)\r\n",
    "\r\n",
    "class BPNet(torch.nn.Module):\r\n",
    "\tdef __init__(self, n_celltypes, n_assays, n_filters=64, n_layers=4, trimming=None):\r\n",
    "\t\tsuper(BPNet, self).__init__()\r\n",
    "\t\tself.trimming = trimming or 2 ** n_layers\r\n",
    "\t\tself.n_filters = n_filters\r\n",
    "\t\tself.n_layers = n_layers\r\n",
    "\t\tself.n_celltypes = n_celltypes\r\n",
    "\t\tself.n_assays = n_assays\r\n",
    "\r\n",
    "\t\tself.iconv = torch.nn.Conv1d(4, n_filters, kernel_size=21, padding=10)\r\n",
    "\t\tself.rconvs = torch.nn.ModuleList([\r\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=3, padding=2**i, dilation=2**i) for i in range(1, self.n_layers+1)\r\n",
    "\t\t])\r\n",
    "\r\n",
    "\t\tself.assay_convs = torch.nn.ModuleList([\r\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=75) for i in range(n_assays)\r\n",
    "\t\t])\r\n",
    "\r\n",
    "\t\tself.celltype_convs = torch.nn.ModuleList([\r\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=75) for i in range(n_celltypes)\r\n",
    "\t\t])\r\n",
    "\r\n",
    "\t\t#self.fconv = torch.nn.Conv1d(n_filters, (n_celltypes + n_assays) * n_filters, kernel_size=75)\r\n",
    "\t\tself.relu = torch.nn.ReLU()\r\n",
    "\t\tself.logsoftmax = torch.nn.LogSoftmax(dim=-1)\r\n",
    "\r\n",
    "\tdef forward(self, X, celltype_idxs, assay_idxs):\r\n",
    "\t\tstart, end = self.trimming, X.shape[2] - self.trimming\r\n",
    "\r\n",
    "\t\tX = self.relu(self.iconv(X))\r\n",
    "\t\tfor i in range(self.n_layers):\r\n",
    "\t\t\tX_conv = self.relu(self.rconvs[i](X))\r\n",
    "\t\t\tX = torch.add(X, X_conv)\r\n",
    "\r\n",
    "\t\tX = X[:, :, start:end]\r\n",
    "\t\t\r\n",
    "\t\tX_celltype, X_assay = [], []\r\n",
    "\t\tfor i, (celltype_idx, assay_idx) in enumerate(zip(celltype_idxs, assay_idxs)):\r\n",
    "\t\t\tXc = self.celltype_convs[celltype_idx](X[i:i+1])\r\n",
    "\t\t\tX_celltype.append(Xc)\r\n",
    "\r\n",
    "\t\t\tXa = self.assay_convs[assay_idx](X[i:i+1])\r\n",
    "\t\t\tX_assay.append(Xa)\r\n",
    "\r\n",
    "\t\tX_celltype = torch.cat(X_celltype)\r\n",
    "\t\tX_assay = torch.cat(X_assay)\r\n",
    "\r\n",
    "\t\ty_profile = torch.mul(X_celltype, X_assay)\r\n",
    "\t\ty_profile = torch.sum(y_profile, dim=1).squeeze()\r\n",
    "\t\ty_profile = self.logsoftmax(y_profile)\r\n",
    "\r\n",
    "\t\t# counts prediction\r\n",
    "\t\t#X_avg = torch.mean(X, axis=2)\r\n",
    "\t\t#y_counts = self.linear(X_avg) \r\n",
    "\t\treturn y_profile\r\n",
    "\r\n",
    "\tdef predict(self, X, celltype_idxs, assay_idxs, batch_size=64):\r\n",
    "\t\twith torch.no_grad():\r\n",
    "\t\t\tstarts = numpy.arange(0, X.shape[0], batch_size)\r\n",
    "\t\t\tends = starts + batch_size\r\n",
    "\r\n",
    "\t\t\ty_hat = []\r\n",
    "\t\t\tfor start, end in zip(starts, ends):\r\n",
    "\t\t\t\ty_hat_ = self(X[start:end], celltype_idxs[start:end],\r\n",
    "\t\t\t\t\tassay_idxs[start:end]).cpu().detach().numpy()\r\n",
    "\t\t\t\ty_hat.append(y_hat_)\r\n",
    "\r\n",
    "\t\t\ty_hat = numpy.concatenate(y_hat)\r\n",
    "\t\t\treturn y_hat\r\n",
    "\r\n",
    "\tdef fit_generator(self, training_data, optimizer, X_valid=None, \r\n",
    "\t\tcelltype_idxs_valid=None, assay_idxs_valid=None, y_valid=None, \r\n",
    "\t\tmax_epochs=100, batch_size=64, validation_iter=100, verbose=True):\r\n",
    "\r\n",
    "\t\tif X_valid is not None: \r\n",
    "\t\t\tX_valid = X_valid.cuda()\r\n",
    "\t\t\tcelltype_idxs_valid = celltype_idxs_valid.cuda()\r\n",
    "\t\t\tassay_idxs_valid = assay_idxs_valid.cuda()\r\n",
    "\t\t\r\n",
    "\t\ty_valid_ = y_valid.detach().numpy()\r\n",
    "\r\n",
    "\t\tif verbose:\r\n",
    "\t\t\tprint(\"Epoch\\tIteration\\tTraining Time\\tValidation Time\\tTraining MLL\\tValidation MLLL\\tValidation Correlation\")\r\n",
    "\r\n",
    "\t\tstart = time.time()\r\n",
    "\t\titeration = 0\r\n",
    "\t\tbest_corr = 0\r\n",
    "\r\n",
    "\t\tfor epoch in range(max_epochs):\r\n",
    "\t\t\ttic = time.time()\r\n",
    "\r\n",
    "\t\t\tfor X, celltype_idxs, assay_idxs, y in training_data:\r\n",
    "\t\t\t\tX = X.cuda()\r\n",
    "\t\t\t\tcelltype_idxs = celltype_idxs.cuda()\r\n",
    "\t\t\t\tassay_idxs = assay_idxs.cuda()\r\n",
    "\t\t\t\ty = y.cuda()\r\n",
    "\r\n",
    "\t\t\t\toptimizer.zero_grad()\r\n",
    "\t\t\t\tself.train()\r\n",
    "\r\n",
    "\t\t\t\ty_profile = self(X, celltype_idxs, assay_idxs)\r\n",
    "\r\n",
    "\t\t\t\tloss = MLLLoss(y_profile, y)\r\n",
    "\t\t\t\ttrain_loss = loss.item()\r\n",
    "\t\t\t\tloss.backward()\r\n",
    "\r\n",
    "\t\t\t\toptimizer.step()\r\n",
    "\r\n",
    "\t\t\t\tif verbose and iteration % validation_iter == 0:\r\n",
    "\t\t\t\t\tself.eval()\r\n",
    "\r\n",
    "\t\t\t\t\ttrain_time = time.time() - start\r\n",
    "\t\t\t\t\ttic = time.time()\r\n",
    "\r\n",
    "\t\t\t\t\ty_profile = self.predict(X_valid, celltype_idxs_valid, assay_idxs_valid, batch_size=batch_size)\r\n",
    "\t\t\t\t\tvalid_loss = MLLLoss(y_profile, y_valid).item()\r\n",
    "\r\n",
    "\t\t\t\t\ty_profile = numpy.exp(y_profile)\r\n",
    "\t\t\t\t\tvalid_corrs = numpy.mean(numpy.nan_to_num(pearson_corr(y_profile, y_valid_)))\r\n",
    "\t\t\t\t\tvalid_time = time.time() - tic\r\n",
    "\r\n",
    "\t\t\t\t\tprint(\"{}\\t{}\\t{:4.4}\\t{:4.4}\\t{:6.6}\\t{:6.6}\\t{:4.4}\".format(\r\n",
    "\t\t\t\t\t\tepoch, iteration, train_time, valid_time, train_loss, valid_loss, \r\n",
    "\t\t\t\t\t\tvalid_corrs))\r\n",
    "\t\t\t\t\tstart = time.time()\r\n",
    "\r\n",
    "\t\t\t\t\tif valid_corrs > best_corr:\r\n",
    "\t\t\t\t\t\tbest_corr = valid_corrs\r\n",
    "\r\n",
    "\t\t\t\t\t\tself = self.cpu()\r\n",
    "\t\t\t\t\t\ttorch.save(self, \"/mnt/data/imputation_yangyuan/models/bpnet.{}.{}.torch\".format(self.n_filters, self.n_layers))\r\n",
    "\t\t\t\t\t\tself = self.cuda()\r\n",
    "\t\t\t\t\r\n",
    "\t\t\t\titeration += 1\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "class PeakGenerator(torch.utils.data.Dataset):\r\n",
    "\tdef __init__(self, sequence, signal, assays, celltypes, peakfiles, trimming, window, chroms, reverse_complement=True, \r\n",
    "\t\trandom_state=None):\r\n",
    "\t\tself.trimming = trimming\r\n",
    "\t\tself.window = window\r\n",
    "\t\tself.chroms = chroms\r\n",
    "\t\tself.reverse_complement = reverse_complement\r\n",
    "\t\tself.random_state = numpy.random.RandomState(random_state)\r\n",
    "\r\n",
    "\t\tself.signal = {chrom: signal[chrom] for chrom in chroms}\r\n",
    "\t\tself.sequence = {chrom: sequence[chrom] for chrom in chroms}\r\n",
    "\r\n",
    "\t\tself.assays = assays\r\n",
    "\t\tself.celltypes = celltypes\r\n",
    "\r\n",
    "\t\tself.tracks = list(signal['chr18'].keys()) \r\n",
    "\r\n",
    "\t\tself.peakfiles = {}\r\n",
    "\t\tfor track, peaks in peakfiles.items():\r\n",
    "\t\t\tpeaks['mid'] = (peaks['end'] - peaks['start']) // 2 + peaks['start']\r\n",
    "\t\t\tself.peakfiles[track] = peaks[numpy.isin(peaks['chrom'], chroms)].reset_index(drop=True)\r\n",
    "\r\n",
    "\t\t#print(\"Training Examples: {}\".format(sum(map(len, self.peakfiles.values()))))\r\n",
    "\t\r\n",
    "\r\n",
    "\tdef __len__(self):\r\n",
    "\t\t#return self.peaks.shape[0]\r\n",
    "\t\treturn sum(map(len, self.peakfiles.values()))\r\n",
    "\r\n",
    "\tdef __getitem__(self, idx):\r\n",
    "\t\ttrack_idx = numpy.random.choice(len(self.tracks))\r\n",
    "\t\ttrack = self.tracks[track_idx]\r\n",
    "\t\tpeaks = self.peakfiles[track]\r\n",
    "\r\n",
    "\t\ti = numpy.random.choice(peaks.shape[0])\r\n",
    "\t\tmid = peaks['mid'][i]\r\n",
    "\t\tmid += self.random_state.randint(-128, 129)\r\n",
    "\t\tstart, end = mid - self.window // 2, mid + self.window // 2\r\n",
    "\t\t\r\n",
    "\t\tchrom = 'chr18'\r\n",
    "\r\n",
    "\t\tcelltype_idx = self.celltypes.index(track[0])\r\n",
    "\t\tassay_idx = self.assays.index(track[1])\r\n",
    "\r\n",
    "\t\tX = self.sequence[chrom][start:end].T\r\n",
    "\t\ty = self.signal[chrom][track][start+trimming:end-trimming]\r\n",
    "\r\n",
    "\t\tif self.reverse_complement and numpy.random.choice(2) == 1:\r\n",
    "\t\t\tX = X[::-1][:, ::-1]\r\n",
    "\t\t\ty = y[::-1]\r\n",
    "\r\n",
    "\t\tX = torch.tensor(X.copy(), dtype=torch.float32)\r\n",
    "\t\ty = torch.tensor(y.copy())\r\n",
    "\t\tcelltype_idx = torch.LongTensor([celltype_idx])\r\n",
    "\t\tassay_idx = torch.LongTensor([assay_idx])\r\n",
    "\t\treturn X, celltype_idx, assay_idx, y\r\n",
    "\r\n",
    "\r\n",
    "def validation_data(sequence, signal, assays, celltypes, peakfiles, trimming=2**4, window=1000, chroms=None, size=3000, random_state=None):\r\n",
    "\tsequence = {chrom: sequence[chrom] for chrom in chroms}\r\n",
    "\tsignal = {chrom: signal[chrom] for chrom in chroms}\r\n",
    "\r\n",
    "\tfor track, peaks in peakfiles.items():\r\n",
    "\t\tpeaks['mid'] = (peaks['end'] - peaks['start']) // 2 + peaks['start']\r\n",
    "\t\tpeakfiles[track] = peaks[numpy.isin(peaks['chrom'], chroms)].reset_index(drop=True)\r\n",
    "\t\r\n",
    "\tX = numpy.zeros((size, 4, window), dtype='float32')\r\n",
    "\ty = numpy.zeros((size, window-trimming*2), dtype='float32')\r\n",
    "\r\n",
    "\tchrom = 'chr18'\r\n",
    "\tn = len(sequence[chrom])\r\n",
    "\ttrack_names = list(signal[chrom].keys())\r\n",
    "\r\n",
    "\trandom_state = numpy.random.RandomState(random_state)\r\n",
    "\r\n",
    "\ttrack_idxs = random_state.choice(len(track_names), size=size)\r\n",
    "\ttracks = [track_names[track_idx] for track_idx in track_idxs]\r\n",
    "\r\n",
    "\tcelltype_idxs = [celltypes.index(celltype) for celltype, _ in tracks]\r\n",
    "\tassay_idxs = [assays.index(assay) for _, assay in tracks]\r\n",
    "\r\n",
    "\tfor i, track in enumerate(tracks):\r\n",
    "\t\tpeaks = peakfiles[track]\r\n",
    "\t\tj = numpy.random.choice(peaks.shape[0])\r\n",
    "\t\tmid = peaks['mid'][j]\r\n",
    "\r\n",
    "\t\tstart, end = mid - window // 2, mid + window // 2\r\n",
    "\r\n",
    "\t\tX[i] = sequence[chrom][start:end].T\r\n",
    "\t\ty[i] = signal[chrom][track][start+trimming:end-trimming]\r\n",
    "\r\n",
    "\tX = torch.tensor(X.copy())\r\n",
    "\ty = torch.tensor(y.copy())\r\n",
    "\tcelltype_idxs = torch.LongTensor(celltype_idxs)\r\n",
    "\tassay_idxs = torch.LongTensor(assay_idxs)\r\n",
    "\treturn X, celltype_idxs, assay_idxs, y\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_filters = 32\r\n",
    "n_layers = 8\r\n",
    "\r\n",
    "\r\n",
    "window = 2114\r\n",
    "trimming = (window - 1000) // 2\r\n",
    "chroms = ['chr18']\r\n",
    "\r\n",
    "training_metadata = pandas.read_csv(\"/users/jmschr/oak/proj/2021_sequence_imputation/scripts/metadata_training_data.tsv\", sep=\"\\t\")\r\n",
    "training_tracks = training_metadata[['cell_type_id', 'mark_id']].values\r\n",
    "\r\n",
    "validation_metadata = pandas.read_csv(\"/users/jmschr/oak/proj/2021_sequence_imputation/scripts/metadata_validation_data.tsv\", sep=\"\\t\")\r\n",
    "validation_tracks = validation_metadata[['cell_type_id', 'mark_id']].values\r\n",
    "\r\n",
    "celltypes = list(numpy.unique(training_tracks[:,0]))\r\n",
    "assays = list(numpy.unique(training_tracks[:,1]))\r\n",
    "\r\n",
    "n_celltypes = len(celltypes)\r\n",
    "n_assays = len(assays)\r\n",
    "\r\n",
    "seq_dir = \"/users/jmschr/oak/common/hg38/\"\r\n",
    "\r\n",
    "###\r\n",
    "# Load single-cell data\r\n",
    "#print(\"Loading data\")\r\n",
    "\r\n",
    "sequence = {}\r\n",
    "training_signal, validation_signal = {}, {}\r\n",
    "training_peaks, validation_peaks = {}, {}\r\n",
    "\r\n",
    "for chrom in chroms:\r\n",
    "\ttraining_signal[chrom] = {}\r\n",
    "\tfor track in tqdm(training_tracks):\r\n",
    "\t\ttrack = tuple(track)\r\n",
    "\t\t\r\n",
    "\t\tsig = numpy.load(\"/users/jmschr/oak/proj/2021_sequence_imputation/data/tracks/{}{}.{}.npy\".format(*track, chrom), mmap_mode='r')\r\n",
    "\t\ttraining_signal[chrom][track] = sig\r\n",
    "\r\n",
    "\t\tnames = ['chrom', 'start', 'end']\r\n",
    "\t\tpeaks = pandas.read_csv(\"/users/jmschr/oak/proj/2021_sequence_imputation/data/peaks/{}{}.bed.gz\".format(*track), sep=\"\\t\", \r\n",
    "\t\t\tusecols=(0, 1, 2), header=None, index_col=False, names=names)\r\n",
    "\t\ttraining_peaks[track] = peaks\r\n",
    "\r\n",
    "\tvalidation_signal[chrom] = {}\r\n",
    "\tfor track in tqdm(validation_tracks):\r\n",
    "\t\ttrack = tuple(track)\r\n",
    "\r\n",
    "\t\tsig = numpy.load(\"/users/jmschr/oak/proj/2021_sequence_imputation/data/tracks/{}{}.{}.npy\".format(*track, chrom), mmap_mode='r')\r\n",
    "\t\tvalidation_signal[chrom][track] = sig\r\n",
    "\r\n",
    "\t\tnames = ['chrom', 'start', 'end']\r\n",
    "\t\tpeaks = pandas.read_csv(\"/users/jmschr/oak/proj/2021_sequence_imputation/data/peaks/{}{}.bed.gz\".format(*track), sep=\"\\t\", \r\n",
    "\t\t\tusecols=(0, 1, 2), header=None, index_col=False, names=names)\r\n",
    "\t\tvalidation_peaks[track] = peaks\r\n",
    "\r\n",
    "\tseq = numpy.load(\"{}/{}.npy\".format(seq_dir, chrom), mmap_mode='r')\r\n",
    "\tsequence[chrom] = seq\r\n",
    "\r\n",
    "\t#print(\"Loaded {}\".format(chrom))\r\n",
    "\r\n",
    "#print(\"Done loading data\")\r\n",
    "###\r\n",
    "\r\n",
    "training_peaks = PeakGenerator(\r\n",
    "\tsequence=sequence,\r\n",
    "\tsignal=training_signal,\r\n",
    "\tassays=assays,\r\n",
    "\tcelltypes=celltypes,\r\n",
    "\tpeakfiles=training_peaks, \r\n",
    "\ttrimming=trimming, \r\n",
    "\twindow=window, \r\n",
    "\tchroms=chroms)\r\n",
    "\r\n",
    "training_data = torch.utils.data.DataLoader(training_peaks, \r\n",
    "\tpin_memory=True, \r\n",
    "\tbatch_size=batch_size)\r\n",
    "\r\n",
    "X_valid, celltype_idxs_valid, assay_idxs_valid, y_valid = validation_data(\r\n",
    "\tsequence=sequence,\r\n",
    "\tsignal=validation_signal,\r\n",
    "\tassays=assays,\r\n",
    "\tcelltypes=celltypes,\r\n",
    "\tpeakfiles=validation_peaks,\r\n",
    "\ttrimming=trimming, \r\n",
    "\twindow=window,\r\n",
    "\tchroms=chroms)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 267/267 [02:53<00:00,  1.54it/s]\n",
      "100%|██████████| 45/45 [00:53<00:00,  1.19s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch\tIteration\tTraining Time\tValidation Time\tTraining MLL\tValidation MLLL\tValidation Correlation\n",
      "0\t0\t5.194\t1.793\t13468.8\t8681.63\t0.0006257\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_size = 64\r\n",
    "\r\n",
    "\r\n",
    "n_layers_list = [7,8,9,10]\r\n",
    "for n_layers in n_layers_list:\r\n",
    "\r\n",
    "\tmodel = BPNet(n_celltypes=n_celltypes, n_assays=n_assays, \r\n",
    "\t\tn_filters=n_filters, n_layers=n_layers, trimming=trimming-37).cuda()\r\n",
    "\r\n",
    "\t#print(n_celltypes, n_assays)\r\n",
    "\r\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=0.002)\r\n",
    "\t#summary(model, [(4, window), (1, 1), (1, 1)], batch_size=batch_size, dtypes=torch.long)\r\n",
    "\r\n",
    "\tmodel.fit_generator(training_data, optimizer, X_valid, celltype_idxs_valid,\r\n",
    "\t\tassay_idxs_valid, y_valid, max_epochs=30, validation_iter=100, \r\n",
    "\t\tbatch_size=batch_size)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}