{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f015e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import pandas\n",
    "import pyBigWig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65693235",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 32\n",
    "n_layers = 11\n",
    "batch_size = 64\n",
    "\n",
    "def MLLLoss(logps, true_counts):\n",
    "\t\"\"\" Adapted from Alex.\n",
    "\t\"\"\"\n",
    "\t# Multinomial probability = n! / (x1!...xk!) * p1^x1 * ... pk^xk\n",
    "\t# Log prob = log(n!) - (log(x1!) ... + log(xk!)) + x1log(p1) ... + xklog(pk)\n",
    "\n",
    "\tlog_fact_sum = torch.lgamma(torch.sum(true_counts, dim=-1) + 1)\n",
    "\tlog_prod_fact = torch.sum(torch.lgamma(true_counts + 1), dim=-1)\n",
    "\tlog_prod_exp = torch.sum(true_counts * logps, dim=-1) \n",
    "\treturn -torch.mean(log_fact_sum - log_prod_fact + log_prod_exp)\n",
    "\n",
    "def pearson_corr(arr1, arr2):\n",
    "\t\"\"\"The Pearson correlation between two draws of samples.\n",
    "\n",
    "\tThis function is more efficient than the built-in `corrcoef` function\n",
    "\tbecause it only calculates the pairwise correlations between elements\n",
    "\tin arr1 and arr2 rather than the correlation between all elements in arr1\n",
    "\tand arr2, and does so in a vectorized manner.\n",
    "\n",
    "\tComputes the Pearson correlation in the last dimension of `arr1` and `arr2`.\n",
    "\t`arr1` and `arr2` must be the same shape. For example, if they are both\n",
    "\tA x B x L arrays, then the correlation of corresponding L-arrays will be\n",
    "\tcomputed and returned in an A x B array.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tarr1: numpy.ndarray, shape=(A, ..., L)\n",
    "\t\tAn array of any dimensionality > 1 as long as the last dimension\n",
    "\t\tcorresponds to a sample from the same distribution.\n",
    "\n",
    "\tarr2: numpy.ndarray, shape=(A, ..., L)\n",
    "\t\tAn array of any dimensionality > 1 as long as the last dimension\n",
    "\t\tcorresponds to a sample from the same distribution.\t\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tcorr : numpy.array, shape=(A, ...)\n",
    "\t\tThe Pearson correlation along the last dimension.\n",
    "\t\"\"\"\n",
    "\n",
    "\tmean1 = numpy.mean(arr1, axis=-1, keepdims=True)\n",
    "\tmean2 = numpy.mean(arr2, axis=-1, keepdims=True)\n",
    "\tdev1, dev2 = arr1 - mean1, arr2 - mean2\n",
    "\tsqdev1, sqdev2 = numpy.square(dev1), numpy.square(dev2)\n",
    "\tnumer = numpy.sum(dev1 * dev2, axis=-1)  # Covariance\n",
    "\tvar1, var2 = numpy.sum(sqdev1, axis=-1), numpy.sum(sqdev2, axis=-1)  # Variances\n",
    "\tdenom = numpy.sqrt(var1 * var2)\n",
    "   \n",
    "\t# Divide numerator by denominator, but use NaN where the denominator is 0\n",
    "\treturn numpy.divide(\n",
    "\t\tnumer, denom, out=numpy.full_like(numer, numpy.nan), where=(denom != 0)\n",
    "\t)\n",
    "\n",
    "class BPNet(torch.nn.Module):\n",
    "\tdef __init__(self, n_celltypes, n_assays, n_filters=64, n_layers=4, trimming=None):\n",
    "\t\tsuper(BPNet, self).__init__()\n",
    "\t\tself.trimming = trimming or 2 ** n_layers\n",
    "\t\tself.n_filters = n_filters\n",
    "\t\tself.n_layers = n_layers\n",
    "\t\tself.n_celltypes = n_celltypes\n",
    "\t\tself.n_assays = n_assays\n",
    "\n",
    "\t\tself.iconv = torch.nn.Conv1d(4, n_filters, kernel_size=21, padding=10)\n",
    "\t\tself.rconvs = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=3, padding=2**i, dilation=2**i) for i in range(1, self.n_layers+1)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.assay_convs = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=75) for i in range(n_assays)\n",
    "\t\t])\n",
    "\n",
    "\t\tself.celltype_convs = torch.nn.ModuleList([\n",
    "\t\t\ttorch.nn.Conv1d(n_filters, n_filters, kernel_size=75) for i in range(n_celltypes)\n",
    "\t\t])\n",
    "\n",
    "\t\t#self.fconv = torch.nn.Conv1d(n_filters, (n_celltypes + n_assays) * n_filters, kernel_size=75)\n",
    "\t\tself.relu = torch.nn.ReLU()\n",
    "\t\tself.logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "\tdef forward(self, X, celltype_idxs, assay_idxs):\n",
    "\t\tstart, end = self.trimming, X.shape[2] - self.trimming\n",
    "\n",
    "\t\tX = self.relu(self.iconv(X))\n",
    "\t\tfor i in range(self.n_layers):\n",
    "\t\t\tX_conv = self.relu(self.rconvs[i](X))\n",
    "\t\t\tX = torch.add(X, X_conv)\n",
    "\n",
    "\t\tX = X[:, :, start:end]\n",
    "\t\t\n",
    "\t\tX_celltype, X_assay = [], []\n",
    "\t\tfor i, (celltype_idx, assay_idx) in enumerate(zip(celltype_idxs, assay_idxs)):\n",
    "\t\t\tXc = self.celltype_convs[celltype_idx](X[i:i+1])\n",
    "\t\t\tX_celltype.append(Xc)\n",
    "\n",
    "\t\t\tXa = self.assay_convs[assay_idx](X[i:i+1])\n",
    "\t\t\tX_assay.append(Xa)\n",
    "\n",
    "\t\tX_celltype = torch.cat(X_celltype)\n",
    "\t\tX_assay = torch.cat(X_assay)\n",
    "\n",
    "\t\ty_profile = torch.mul(X_celltype, X_assay)\n",
    "\t\ty_profile = torch.sum(y_profile, dim=1).squeeze()\n",
    "\t\ty_profile = self.logsoftmax(y_profile)\n",
    "\n",
    "\t\t# counts prediction\n",
    "\t\t#X_avg = torch.mean(X, axis=2)\n",
    "\t\t#y_counts = self.linear(X_avg) \n",
    "\t\treturn y_profile\n",
    "\n",
    "\tdef predict(self, X, celltype_idxs, assay_idxs, batch_size=64):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tstarts = numpy.arange(0, X.shape[0], batch_size)\n",
    "\t\t\tends = starts + batch_size\n",
    "\n",
    "\t\t\ty_hat = []\n",
    "\t\t\tfor start, end in zip(starts, ends):\n",
    "\t\t\t\ty_hat_ = self(X[start:end], celltype_idxs[start:end],\n",
    "\t\t\t\t\tassay_idxs[start:end]).cpu().detach().numpy()\n",
    "\t\t\t\ty_hat.append(y_hat_)\n",
    "\n",
    "\t\t\ty_hat = numpy.concatenate(y_hat)\n",
    "\t\t\treturn y_hat\n",
    "\n",
    "\tdef fit_generator(self, training_data, optimizer, X_valid=None, \n",
    "\t\tcelltype_idxs_valid=None, assay_idxs_valid=None, y_valid=None, \n",
    "\t\tmax_epochs=100, batch_size=64, validation_iter=100, verbose=True):\n",
    "\n",
    "\t\tif X_valid is not None: \n",
    "\t\t\tX_valid = X_valid.cuda()\n",
    "\t\t\tcelltype_idxs_valid = celltype_idxs_valid.cuda()\n",
    "\t\t\tassay_idxs_valid = assay_idxs_valid.cuda()\n",
    "\t\t\n",
    "\t\ty_valid_ = y_valid.detach().numpy()\n",
    "\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(\"Epoch\\tIteration\\tTraining Time\\tValidation Time\\tTraining MLL\\tValidation MLLL\\tValidation Correlation\")\n",
    "\n",
    "\t\tstart = time.time()\n",
    "\t\titeration = 0\n",
    "\t\tbest_corr = 0\n",
    "\n",
    "\t\tfor epoch in range(max_epochs):\n",
    "\t\t\ttic = time.time()\n",
    "\n",
    "\t\t\tfor X, celltype_idxs, assay_idxs, y in training_data:\n",
    "\t\t\t\tX = X.cuda()\n",
    "\t\t\t\tcelltype_idxs = celltype_idxs.cuda()\n",
    "\t\t\t\tassay_idxs = assay_idxs.cuda()\n",
    "\t\t\t\ty = y.cuda()\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tself.train()\n",
    "\n",
    "\t\t\t\ty_profile = self(X, celltype_idxs, assay_idxs)\n",
    "\n",
    "\t\t\t\tloss = MLLLoss(y_profile, y)\n",
    "\t\t\t\ttrain_loss = loss.item()\n",
    "\t\t\t\tloss.backward()\n",
    "\n",
    "\t\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t\tif verbose and iteration % validation_iter == 0:\n",
    "\t\t\t\t\tself.eval()\n",
    "\n",
    "\t\t\t\t\ttrain_time = time.time() - start\n",
    "\t\t\t\t\ttic = time.time()\n",
    "\n",
    "\t\t\t\t\ty_profile = self.predict(X_valid, celltype_idxs_valid, assay_idxs_valid, batch_size=batch_size)\n",
    "\t\t\t\t\tvalid_loss = MLLLoss(y_profile, y_valid).item()\n",
    "\n",
    "\t\t\t\t\ty_profile = numpy.exp(y_profile)\n",
    "\t\t\t\t\tvalid_corrs = numpy.mean(numpy.nan_to_num(pearson_corr(y_profile, y_valid_)))\n",
    "\t\t\t\t\tvalid_time = time.time() - tic\n",
    "\n",
    "\t\t\t\t\tprint(\"{}\\t{}\\t{:4.4}\\t{:4.4}\\t{:6.6}\\t{:6.6}\\t{:4.4}\".format(\n",
    "\t\t\t\t\t\tepoch, iteration, train_time, valid_time, train_loss, valid_loss, \n",
    "\t\t\t\t\t\tvalid_corrs))\n",
    "\t\t\t\t\tstart = time.time()\n",
    "\n",
    "\t\t\t\t\tif valid_corrs > best_corr:\n",
    "\t\t\t\t\t\tbest_corr = valid_corrs\n",
    "\n",
    "\t\t\t\t\t\tself = self.cpu()\n",
    "\t\t\t\t\t\ttorch.save(self, \"/mnt/data/imputation_yangyuan/models/bpnet.{}.{}.torch\".format(self.n_filters, self.n_layers))\n",
    "\t\t\t\t\t\tself = self.cuda()\n",
    "\t\t\t\t\n",
    "\t\t\t\titeration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b8446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"/mnt/data/imputation_yangyuan/models/bpnet.{}.{}.torch\".format(n_filters, n_layers))\n",
    "X_valid, celltype_idxs_valid, assay_idxs_valid, y_valid = torch.load('/mnt/data/imputation_yangyuan/data/tensor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1cd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16879.48046875 0.6765633\n",
      "2178.964111328125 0.61263734\n",
      "1305.018798828125 0.5102154\n",
      "skip\n",
      "skip\n",
      "skip\n",
      "1436.2794189453125 0.5518573\n",
      "skip\n",
      "2322.6044921875 0.5954517\n",
      "skip\n",
      "1464.83251953125 0.6632016\n",
      "1682.96484375 0.64920574\n",
      "skip\n",
      "skip\n",
      "2464.652587890625 0.5142322\n",
      "1823.1298828125 0.5998121\n",
      "1801.93896484375 0.5042928\n",
      "skip\n",
      "1681.7593994140625 0.43325308\n",
      "2827.552978515625 0.31931686\n",
      "3818.188720703125 0.46739948\n",
      "skip\n",
      "1586.700927734375 0.46862066\n",
      "2059.775390625 0.3971115\n",
      "3414.522216796875 0.49428535\n",
      "skip\n",
      "skip\n",
      "1568.4915771484375 0.3894616\n",
      "skip\n",
      "skip\n",
      "1661.671142578125 0.46577382\n",
      "skip\n",
      "skip\n",
      "2507.698486328125 0.61141855\n"
     ]
    }
   ],
   "source": [
    "model.cpu()\n",
    "for i in range(1,max(assay_idxs_valid)+1):\n",
    "    index_list = torch.where(assay_idxs_valid==i)\n",
    "    X_valid_sel = X_valid[index_list]\n",
    "    y_valid_sel = y_valid[index_list]\n",
    "    celltype_idxs_valid_sel = celltype_idxs_valid[index_list]\n",
    "    assay_idxs_valid_sel = assay_idxs_valid[index_list]\n",
    "    try:\n",
    "        y_profile_sel = model.predict(X_valid_sel, celltype_idxs_valid_sel, assay_idxs_valid_sel, batch_size=batch_size)\n",
    "        valid_loss = MLLLoss(y_profile_sel, y_valid_sel).item()\n",
    "        y_profile_sel = numpy.exp(y_profile_sel)\n",
    "        y_valid_sel_ = y_valid_sel.detach().numpy()\n",
    "        valid_corrs = numpy.mean(numpy.nan_to_num(pearson_corr(y_profile_sel, y_valid_sel_)))\n",
    "        print(valid_loss, valid_corrs)\n",
    "    except ValueError:\n",
    "        print('skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9571cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
